{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf3eb29b",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-09-07T00:43:58.128778Z",
     "iopub.status.busy": "2021-09-07T00:43:58.127204Z",
     "iopub.status.idle": "2021-09-07T00:44:04.588451Z",
     "shell.execute_reply": "2021-09-07T00:44:04.589174Z",
     "shell.execute_reply.started": "2021-09-07T00:35:26.141099Z"
    },
    "papermill": {
     "duration": 6.47512,
     "end_time": "2021-09-07T00:44:04.589480",
     "exception": false,
     "start_time": "2021-09-07T00:43:58.114360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/roberta-transformers-pytorch/roberta-large-mnli/config.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large-mnli/merges.txt\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large-mnli/vocab.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large-mnli/tokenizer_config.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large-mnli/pytorch_model.bin\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large-mnli/special_tokens_map.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large-mnli/added_tokens.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-base/config.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-base/merges.txt\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-base/vocab.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-base/tokenizer_config.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-base/pytorch_model.bin\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-base/special_tokens_map.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-base/added_tokens.json\n",
      "/kaggle/input/roberta-transformers-pytorch/distilroberta-base/config.json\n",
      "/kaggle/input/roberta-transformers-pytorch/distilroberta-base/merges.txt\n",
      "/kaggle/input/roberta-transformers-pytorch/distilroberta-base/vocab.json\n",
      "/kaggle/input/roberta-transformers-pytorch/distilroberta-base/tokenizer_config.json\n",
      "/kaggle/input/roberta-transformers-pytorch/distilroberta-base/pytorch_model.bin\n",
      "/kaggle/input/roberta-transformers-pytorch/distilroberta-base/special_tokens_map.json\n",
      "/kaggle/input/roberta-transformers-pytorch/distilroberta-base/added_tokens.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large/config.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large/merges.txt\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large/vocab.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large/tokenizer_config.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large/pytorch_model.bin\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large/special_tokens_map.json\n",
      "/kaggle/input/roberta-transformers-pytorch/roberta-large/added_tokens.json\n",
      "/kaggle/input/ex014-model-weight/ex014_2.pth\n",
      "/kaggle/input/ex014-model-weight/nlp_shawon_first_model_fold_2.pickle\n",
      "/kaggle/input/commonlitreadabilityprize/sample_submission.csv\n",
      "/kaggle/input/commonlitreadabilityprize/train.csv\n",
      "/kaggle/input/commonlitreadabilityprize/test.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import RobertaModel\n",
    "from transformers import RobertaTokenizer\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1699339",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:44:04.610856Z",
     "iopub.status.busy": "2021-09-07T00:44:04.610159Z",
     "iopub.status.idle": "2021-09-07T00:44:04.761487Z",
     "shell.execute_reply": "2021-09-07T00:44:04.760976Z",
     "shell.execute_reply.started": "2021-09-07T00:35:28.701889Z"
    },
    "papermill": {
     "duration": 0.163231,
     "end_time": "2021-09-07T00:44:04.761642",
     "exception": false,
     "start_time": "2021-09-07T00:44:04.598411",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Settings\n",
    "# ===============\n",
    "SEED = 0\n",
    "num_workers = 4\n",
    "BATCH_SIZE = 24\n",
    "max_len = 256\n",
    "MODEL_PATH = '/kaggle/input/roberta-transformers-pytorch/roberta-base'\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_PATH)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fed7857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:44:04.789962Z",
     "iopub.status.busy": "2021-09-07T00:44:04.788688Z",
     "iopub.status.idle": "2021-09-07T00:44:04.791405Z",
     "shell.execute_reply": "2021-09-07T00:44:04.790996Z",
     "shell.execute_reply.started": "2021-09-07T00:35:28.814402Z"
    },
    "papermill": {
     "duration": 0.0214,
     "end_time": "2021-09-07T00:44:04.791513",
     "exception": false,
     "start_time": "2021-09-07T00:44:04.770113",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ===============\n",
    "# Functions\n",
    "# ===============\n",
    "\n",
    "class CommonLitDataset(Dataset):\n",
    "    def __init__(self, excerpt, tokenizer, max_len, target=None):\n",
    "        self.excerpt = excerpt\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.target = target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.excerpt)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.excerpt[item])\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "        if self.target is not None:\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                \"target\": torch.tensor(self.target[item], dtype=torch.float32)\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"input_ids\": torch.tensor(ids, dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(mask, dtype=torch.long),\n",
    "                \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long)\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "508fbe51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:44:04.815219Z",
     "iopub.status.busy": "2021-09-07T00:44:04.814454Z",
     "iopub.status.idle": "2021-09-07T00:44:04.817127Z",
     "shell.execute_reply": "2021-09-07T00:44:04.816690Z",
     "shell.execute_reply.started": "2021-09-07T00:35:28.830016Z"
    },
    "papermill": {
     "duration": 0.017755,
     "end_time": "2021-09-07T00:44:04.817238",
     "exception": false,
     "start_time": "2021-09-07T00:44:04.799483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class roberta_model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(roberta_model, self).__init__()\n",
    "        self.roberta = RobertaModel.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "        )\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.fc = nn.Linear(768, 256)\n",
    "        self.layernorm = nn.LayerNorm(256)\n",
    "        self.drop2 = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.out = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        # pooler\n",
    "        emb = self.roberta(ids, attention_mask=mask, token_type_ids=token_type_ids)[\n",
    "            'pooler_output']\n",
    "        output = self.drop(emb)\n",
    "        output = self.fc(output)\n",
    "        output = self.layernorm(output)\n",
    "        output = self.drop2(output)\n",
    "        output = self.relu(output)\n",
    "        output = self.out(output)\n",
    "        return output, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b52b42b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:44:04.837002Z",
     "iopub.status.busy": "2021-09-07T00:44:04.836452Z",
     "iopub.status.idle": "2021-09-07T00:44:23.462064Z",
     "shell.execute_reply": "2021-09-07T00:44:23.463035Z",
     "shell.execute_reply.started": "2021-09-07T00:35:28.841464Z"
    },
    "papermill": {
     "duration": 18.637991,
     "end_time": "2021-09-07T00:44:23.463251",
     "exception": false,
     "start_time": "2021-09-07T00:44:04.825260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "model = roberta_model()\n",
    "model.load_state_dict(torch.load(\n",
    "    \"/kaggle/input/ex014-model-weight/ex014_2.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"loaded model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6c1332d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:44:23.502769Z",
     "iopub.status.busy": "2021-09-07T00:44:23.501990Z",
     "iopub.status.idle": "2021-09-07T00:44:49.245475Z",
     "shell.execute_reply": "2021-09-07T00:44:49.244918Z",
     "shell.execute_reply.started": "2021-09-07T00:35:33.221372Z"
    },
    "papermill": {
     "duration": 25.76849,
     "end_time": "2021-09-07T00:44:49.245597",
     "exception": false,
     "start_time": "2021-09-07T00:44:23.477107",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/commonlitreadabilityprize/test.csv\")\n",
    "y = train[\"target\"]\n",
    "fold = 2\n",
    "x_train, y_train = train.iloc[:], y.iloc[:]\n",
    "x_test = test.iloc[:]\n",
    "\n",
    "\n",
    "\n",
    "train_ = CommonLitDataset(\n",
    "         x_train[\"excerpt\"].values, tokenizer, max_len, y_train.values.reshape(-1, 1))\n",
    "test_ = CommonLitDataset(\n",
    "         x_test[\"excerpt\"].values, tokenizer, max_len)\n",
    "\n",
    "\n",
    "# loader\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_, batch_size=BATCH_SIZE, shuffle=False, num_workers=4)\n",
    "\n",
    "\n",
    "\n",
    "# make embedding\n",
    "train_emb = np.ndarray((0, 768))\n",
    "with torch.no_grad():\n",
    "    for d in train_loader:\n",
    "        # =========================\n",
    "        # data loader\n",
    "        # =========================\n",
    "        input_ids = d['input_ids']\n",
    "        mask = d['attention_mask']\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        target = d[\"target\"]\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "        target = target.to(device)\n",
    "        _, emb = model(input_ids, mask, token_type_ids)\n",
    "        train_emb = np.concatenate(\n",
    "            [train_emb, emb.detach().cpu().numpy()], axis=0)\n",
    "\n",
    "        \n",
    "test_emb = np.ndarray((0, 768))\n",
    "with torch.no_grad():\n",
    "    for d in test_loader:\n",
    "        # =========================\n",
    "        # data loader\n",
    "        # =========================\n",
    "        input_ids = d['input_ids']\n",
    "        mask = d['attention_mask']\n",
    "        token_type_ids = d[\"token_type_ids\"]\n",
    "        input_ids = input_ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        token_type_ids = token_type_ids.to(device)\n",
    "\n",
    "        _, emb = model(input_ids, mask, token_type_ids)\n",
    "        test_emb = np.concatenate(\n",
    "            [test_emb, emb.detach().cpu().numpy()], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed1b341b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:44:49.288141Z",
     "iopub.status.busy": "2021-09-07T00:44:49.285929Z",
     "iopub.status.idle": "2021-09-07T00:44:49.296276Z",
     "shell.execute_reply": "2021-09-07T00:44:49.296942Z",
     "shell.execute_reply.started": "2021-09-07T00:35:58.411360Z"
    },
    "papermill": {
     "duration": 0.041221,
     "end_time": "2021-09-07T00:44:49.297127",
     "exception": false,
     "start_time": "2021-09-07T00:44:49.255906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train = train_emb.copy()\n",
    "Y_train = y_train.values.reshape(-1, 1).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14389059",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:44:49.329731Z",
     "iopub.status.busy": "2021-09-07T00:44:49.328203Z",
     "iopub.status.idle": "2021-09-07T00:46:28.952957Z",
     "shell.execute_reply": "2021-09-07T00:46:28.953584Z",
     "shell.execute_reply.started": "2021-09-07T00:35:58.428413Z"
    },
    "papermill": {
     "duration": 99.644136,
     "end_time": "2021-09-07T00:46:28.953940",
     "exception": false,
     "start_time": "2021-09-07T00:44:49.309804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "kernel = RBF()\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, random_state=0).fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc4b006",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:46:28.978281Z",
     "iopub.status.busy": "2021-09-07T00:46:28.977070Z",
     "iopub.status.idle": "2021-09-07T00:46:28.995446Z",
     "shell.execute_reply": "2021-09-07T00:46:28.996005Z",
     "shell.execute_reply.started": "2021-09-07T00:37:36.242257Z"
    },
    "papermill": {
     "duration": 0.032468,
     "end_time": "2021-09-07T00:46:28.996154",
     "exception": false,
     "start_time": "2021-09-07T00:46:28.963686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_test = test_emb.copy()\n",
    "Y_test = Ymean_test = gpr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2fc9e6f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-09-07T00:46:29.034209Z",
     "iopub.status.busy": "2021-09-07T00:46:29.033387Z",
     "iopub.status.idle": "2021-09-07T00:46:29.043851Z",
     "shell.execute_reply": "2021-09-07T00:46:29.044539Z",
     "shell.execute_reply.started": "2021-09-07T00:37:36.264869Z"
    },
    "papermill": {
     "duration": 0.033273,
     "end_time": "2021-09-07T00:46:29.044718",
     "exception": false,
     "start_time": "2021-09-07T00:46:29.011445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sub = pd.DataFrame()\n",
    "df_sub['id'] = test['id']\n",
    "df_sub['target'] = Y_test\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 159.657746,
   "end_time": "2021-09-07T00:46:31.208936",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-09-07T00:43:51.551190",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
